<!-- Advanced Explainable AI Algorithms PhD-Level -->

<style>
    .xai-algorithm-box {
        background: linear-gradient(135deg, #ede9fe, #ddd6fe);
        border: 2px solid #7c3aed;
        border-radius: 1rem;
        padding: 2rem;
        margin: 2rem 0;
    }
    .shap-box {
        background: linear-gradient(135deg, #f0f9ff, #e0f2fe);
        border: 2px solid #0284c7;
        border-radius: 1rem;
        padding: 2rem;
        margin: 2rem 0;
    }
    .lime-box {
        background: linear-gradient(135deg, #f0fdf4, #dcfce7);
        border: 2px solid #16a34a;
        border-radius: 1rem;
        padding: 2rem;
        margin: 2rem 0;
    }
    .causal-box {
        background: linear-gradient(135deg, #fee2e2, #fecaca);
        border: 2px solid #dc2626;
        border-radius: 1rem;
        padding: 2rem;
        margin: 2rem 0;
    }
    .neural-box {
        background: linear-gradient(135deg, #e0e7ff, #c7d2fe);
        border: 2px solid #4f46e5;
        border-radius: 1rem;
        padding: 2rem;
        margin: 2rem 0;
    }
    .algorithm-code {
        background: #1e293b;
        color: #e2e8f0;
        border-radius: 0.5rem;
        padding: 1.5rem;
        margin: 1.5rem 0;
        font-family: 'Courier New', monospace;
        font-size: 0.9rem;
        overflow-x: auto;
    }
    .axiom-list {
        background: white;
        border-radius: 0.5rem;
        padding: 1.5rem;
        margin: 1.5rem 0;
        border: 1px solid #e2e8f0;
    }
    .complexity-note {
        background: #fef3c7;
        border: 1px solid #f59e0b;
        border-radius: 0.5rem;
        padding: 1rem;
        margin: 1rem 0;
        font-size: 0.9rem;
    }
</style>

<!-- Main Title -->
<h3 style="font-size: 1.25rem; font-weight: 600; color: var(--text-primary); margin: 3rem 0 1.5rem 0;">
    üî¨ Advanced Explainable AI Algorithms
</h3>

<p style="color: var(--text-secondary); margin-bottom: 2rem; line-height: 1.6;">
    Rigorous implementation of XAI algorithms with solid mathematical foundations, ensuring complete explainability of decisions from the Cidad√£o.AI multi-agent system.
</p>

<!-- SHAP with Rigorous Foundations -->
<div class="shap-box">
    <h4 style="color: #0c4a6e; margin-bottom: 1.5rem;">SHAP: Complete Axiomatization and Implementation</h4>
    
    <div class="axiom-list">
        <p><strong>Shapley Axioms (1953) - Theoretical Base:</strong></p>
        <ol style="margin: 1rem 0; line-height: 1.8;">
            <li><strong>Efficiency:</strong> Œ£·µ¢‚Çå‚ÇÅ‚Åø œÜ·µ¢(v) = v(N) - v(‚àÖ)</li>
            <li><strong>Symmetry:</strong> If i,j are substitutable in v, then œÜ·µ¢(v) = œÜ‚±º(v)</li>
            <li><strong>Dummy:</strong> If i is dummy player, then œÜ·µ¢(v) = v({i}) - v(‚àÖ)</li>
            <li><strong>Additivity:</strong> œÜ·µ¢(v + w) = œÜ·µ¢(v) + œÜ·µ¢(w)</li>
        </ol>
        
        <p style="margin-top: 1.5rem;"><strong>Uniqueness Theorem (Shapley, 1953):</strong></p>
        <p>There exists a unique function œÜ: 2·¥∫ ‚Üí ‚Ñù‚Åø satisfying the axioms above:</p>
        <div style="text-align: center; margin: 1.5rem 0; font-family: 'Times New Roman', serif; font-size: 1.1rem;">
            œÜ·µ¢(v) = Œ£<sub>S‚äÜN\{i}</sub> [|S|!(n-|S|-1)!/n!] √ó [v(S‚à™{i}) - v(S)]
        </div>
    </div>
    
    <p><strong>Computational Implementation in Cidad√£o.AI:</strong></p>
    
    <div class="algorithm-code">
def TreeSHAP_optimized(model, X, feature_perturbation='tree_path_dependent'):
    """
    Optimized TreeSHAP implementation for ensemble models
    Complexity: O(TLD¬≤) where T=trees, L=leaves, D=depth
    """
    def traverse_tree(tree, x, path_idx=0):
        node = tree.nodes[path_idx]
        
        if node.is_leaf():
            return node.value
        
        # Tree path dependent perturbation
        if x[node.feature] <= node.threshold:
            left_contrib = traverse_tree(tree, x, node.left_child)
            right_contrib = conditional_expectation(tree, node.right_subtree, x)
            
            # Marginal contribution calculation
            return left_contrib + (right_contrib - left_contrib) * path_probability
        else:
            # Mirror calculation for right path
            right_contrib = traverse_tree(tree, x, node.right_child)
            left_contrib = conditional_expectation(tree, node.left_subtree, x)
            return right_contrib + (left_contrib - right_contrib) * path_probability
    
    # Aggregate across all trees in ensemble
    shap_values = np.zeros((len(X), X.shape[1]))
    for tree_idx, tree in enumerate(model.estimators_):
        tree_shap = compute_tree_shap(tree, X)
        shap_values += tree_shap / len(model.estimators_)
    
    return shap_values
    </div>
    
    <div class="complexity-note">
        <strong>Guaranteed Complexity:</strong> O(TLD¬≤) for T trees, L maximum leaves, D maximum depth. 
        For Cidad√£o.AI system with Random Forest of 100 trees, depth 10: ~O(10‚Åµ) operations per explanation.
    </div>
    
    <p><strong>Application in InvestigatorAgent:</strong></p>
    <ul style="margin-left: 1.5rem; line-height: 1.6;">
        <li>‚Ä¢ Explains anomaly classification decisions in public contracts</li>
        <li>‚Ä¢ Identifies critical features: value, frequency, supplier history</li>
        <li>‚Ä¢ Generates visualizations for auditors with mathematical guarantees</li>
        <li>‚Ä¢ Satisfies efficiency property: sum of contributions = prediction</li>
    </ul>
</div>

<!-- LIME for Local Explanations -->
<div class="lime-box">
    <h4 style="color: #14532d; margin-bottom: 1.5rem;">LIME: Local Interpretable Model-agnostic Explanations</h4>
    
    <div class="axiom-list">
        <p><strong>Mathematical Formulation:</strong></p>
        <div style="text-align: center; margin: 1.5rem 0; font-family: 'Times New Roman', serif; font-size: 1.1rem;">
            Œæ(x) = argmin<sub>g‚ààG</sub> L(f, g, œÄ<sub>x</sub>) + Œ©(g)
        </div>
        <p>where:</p>
        <ul style="margin: 1rem 0; line-height: 1.6;">
            <li>‚Ä¢ <strong>L(f, g, œÄ<sub>x</sub>):</strong> Loss function between complex model f and explanation g</li>
            <li>‚Ä¢ <strong>œÄ<sub>x</sub>(z):</strong> Proximity measure from z to x (locality kernel)</li>
            <li>‚Ä¢ <strong>Œ©(g):</strong> Complexity measure of explanation g</li>
            <li>‚Ä¢ <strong>G:</strong> Class of interpretable models (e.g., linear regression)</li>
        </ul>
        
        <p style="margin-top: 1.5rem;"><strong>Locality Kernel:</strong></p>
        <div style="text-align: center; margin: 1rem 0;">
            œÄ<sub>x</sub>(z) = exp(-D(x,z)¬≤/œÉ¬≤)
        </div>
        <p>where D(x,z) is a distance metric appropriate to the domain.</p>
    </div>
    
    <div class="algorithm-code">
def LIME_tabular_optimized(model, instance, feature_names, num_samples=5000):
    """
    Optimized LIME for tabular data (contracts/biddings)
    """
    # Generate neighborhood around instance
    neighborhood = generate_neighborhood(instance, num_samples)
    
    # Compute distances and weights
    distances = pairwise_distances(neighborhood, instance.reshape(1, -1), 
                                 metric='cosine').ravel()
    weights = np.exp(-(distances**2) / (0.25**2))  # œÉ = 0.25
    
    # Get model predictions for neighborhood
    predictions = model.predict_proba(neighborhood)[:, 1]  # probability of anomaly
    
    # Fit interpretable model (Ridge regression for stability)
    interpretable_model = Ridge(alpha=1.0, fit_intercept=True)
    interpretable_model.fit(neighborhood, predictions, sample_weight=weights)
    
    # Extract feature importance
    feature_importance = dict(zip(feature_names, interpretable_model.coef_))
    
    # Compute fidelity score
    local_predictions = interpretable_model.predict(neighborhood)
    fidelity = r2_score(predictions, local_predictions, sample_weight=weights)
    
    return {
        'feature_importance': feature_importance,
        'intercept': interpretable_model.intercept_,
        'fidelity': fidelity,
        'neighborhood_size': num_samples
    }
    </div>
    
    <p><strong>Theoretical Guarantees:</strong></p>
    <ul style="margin-left: 1.5rem; line-height: 1.6;">
        <li>‚Ä¢ <strong>Local Approximation:</strong> ‚Äñf(x) - g(x)‚Äñ<sub>œÄ</sub> ‚â§ Œµ with probability ‚â• 1-Œ¥</li>
        <li>‚Ä¢ <strong>Sample Complexity:</strong> n ‚â• O((d/Œµ¬≤)log(d/Œ¥)) for dimension d</li>
        <li>‚Ä¢ <strong>Fidelity:</strong> R¬≤ > 0.8 in local neighborhood guaranteed</li>
    </ul>
    
    <p><strong>Usage in AnalystAgent:</strong></p>
    <ul style="margin-left: 1.5rem; line-height: 1.6;">
        <li>‚Ä¢ Explains local decisions in specific contract analyses</li>
        <li>‚Ä¢ Generates interpretable linear approximations for auditors</li>
        <li>‚Ä¢ Allows debugging of particular decisions</li>
        <li>‚Ä¢ Identifies most influential features for each case</li>
    </ul>
</div>

<!-- Causal Inference and Counterfactuals -->
<div class="causal-box">
    <h4 style="color: #7f1d1d; margin-bottom: 1.5rem;">Causal Inference and Counterfactual Explanations</h4>
    
    <div class="axiom-list">
        <p><strong>Structural Causal Model (SCM):</strong></p>
        <p>Let M = ‚ü®U, V, F, P(U)‚ü© be a causal model where:</p>
        <ul style="margin: 1rem 0; line-height: 1.8;">
            <li>‚Ä¢ <strong>U:</strong> Exogenous variables (unobserved, noise)</li>
            <li>‚Ä¢ <strong>V = {V‚ÇÅ, V‚ÇÇ, ..., V‚Çô}:</strong> Endogenous variables (observed)</li>
            <li>‚Ä¢ <strong>F = {f‚ÇÅ, f‚ÇÇ, ..., f‚Çô}:</strong> Structural functions V·µ¢ = f·µ¢(pa(V·µ¢), U·µ¢)</li>
            <li>‚Ä¢ <strong>P(U):</strong> Probability distribution over exogenous variables</li>
        </ul>
        
        <p style="margin-top: 1.5rem;"><strong>Counterfactual Query:</strong></p>
        <div style="text-align: center; margin: 1.5rem 0; font-family: 'Times New Roman', serif;">
            P(Y<sub>x</sub> = y | E = e) = Œ£<sub>u</sub> P(Y = y | do(X = x), U = u) √ó P(U = u | E = e)
        </div>
    </div>
    
    <div class="algorithm-code">
def generate_counterfactual_explanation(instance, model, target_class, 
                                      feature_constraints=None, lambda_reg=0.1):
    """
    Optimized counterfactual explanation generation
    """
    x_original = instance.copy()
    x_cf = x_original.copy()
    
    # Define loss function: prediction loss + distance penalty
    def loss_function(x_candidate):
        # Prediction loss (cross-entropy)
        pred_prob = model.predict_proba(x_candidate.reshape(1, -1))[0]
        pred_loss = -np.log(pred_prob[target_class] + 1e-8)
        
        # Distance penalty (L1 + L2 for sparsity and smoothness)
        l1_dist = np.sum(np.abs(x_candidate - x_original))
        l2_dist = np.sum((x_candidate - x_original)**2)
        distance_penalty = lambda_reg * (0.5 * l1_dist + 0.5 * l2_dist)
        
        return pred_loss + distance_penalty
    
    # Constraint handling for valid feature ranges
    bounds = []
    for i, feature_name in enumerate(feature_names):
        if feature_constraints and feature_name in feature_constraints:
            bounds.append(feature_constraints[feature_name])
        else:
            # Use data-driven bounds (5th-95th percentile)
            bounds.append((X_train[:, i].quantile(0.05), 
                          X_train[:, i].quantile(0.95)))
    
    # Optimization with L-BFGS-B (handles bounds efficiently)
    result = minimize(loss_function, x_cf, method='L-BFGS-B', 
                     bounds=bounds, options={'maxiter': 1000})
    
    x_counterfactual = result.x
    
    # Verify counterfactual quality
    cf_prediction = model.predict(x_counterfactual.reshape(1, -1))[0]
    distance = np.linalg.norm(x_counterfactual - x_original)
    validity = (cf_prediction == target_class)
    
    return {
        'counterfactual': x_counterfactual,
        'original_prediction': model.predict(x_original.reshape(1, -1))[0],
        'cf_prediction': cf_prediction,
        'distance': distance,
        'validity': validity,
        'feature_changes': x_counterfactual - x_original
    }
    </div>
    
    <p><strong>System Application:</strong></p>
    <ul style="margin-left: 1.5rem; line-height: 1.6;">
        <li>‚Ä¢ <strong>Question:</strong> "What would change for this contract NOT to be classified as suspicious?"</li>
        <li>‚Ä¢ <strong>Answer:</strong> "If the value were 15% lower AND the supplier had a clean history"</li>
        <li>‚Ä¢ <strong>Validation:</strong> Minimal necessary changes respecting legal constraints</li>
    </ul>
</div>

<!-- Neural Network Interpretability -->
<div class="neural-box">
    <h4 style="color: #312e81; margin-bottom: 1.5rem;">Deep Neural Network Interpretability</h4>
    
    <div class="axiom-list">
        <p><strong>Integrated Gradients (Sundararajan et al., 2017):</strong></p>
        <div style="text-align: center; margin: 1.5rem 0; font-family: 'Times New Roman', serif;">
            IG<sub>i</sub>(x) = (x<sub>i</sub> - x'<sub>i</sub>) √ó ‚à´‚ÇÄ¬π ‚àÇf(x' + Œ±(x - x'))/‚àÇx<sub>i</sub> dŒ±
        </div>
        
        <p><strong>Guaranteed Theoretical Properties:</strong></p>
        <ol style="margin: 1rem 0; line-height: 1.8;">
            <li><strong>Sensitivity:</strong> If f(x) ‚â† f(x') and x<sub>i</sub> ‚â† x'<sub>i</sub>, then IG<sub>i</sub> ‚â† 0</li>
            <li><strong>Implementation Invariance:</strong> Independent of network parameterization</li>
            <li><strong>Completeness:</strong> Œ£·µ¢ IG·µ¢(x) = f(x) - f(x') (total conservation)</li>
        </ol>
        
        <p style="margin-top: 1.5rem;"><strong>Layer-wise Relevance Propagation (LRP):</strong></p>
        <div style="text-align: center; margin: 1rem 0;">
            R<sub>i</sub><sup>(l)</sup> = Œ£‚±º (a<sub>i</sub>w<sub>ij</sub>/Œ£‚Çña<sub>k</sub>w<sub>kj</sub>) √ó R<sub>j</sub><sup>(l+1)</sup>
        </div>
        <p>with conservation property: Œ£·µ¢ R<sub>i</sub><sup>(l)</sup> = Œ£‚±º R<sub>j</sub><sup>(l+1)</sup> = f(x)</p>
    </div>
    
    <div class="algorithm-code">
def integrated_gradients_batched(model, inputs, baseline=None, steps=50):
    """
    Batched Integrated Gradients with optimized processing
    """
    if baseline is None:
        baseline = torch.zeros_like(inputs)
    
    # Generate interpolation path
    alphas = torch.linspace(0, 1, steps + 1)[1:]  # Exclude 0
    alphas = alphas.view(-1, 1, 1)  # Broadcast to batch dimension
    
    # Compute interpolated points
    interpolated = baseline.unsqueeze(0) + alphas * (inputs.unsqueeze(0) - baseline.unsqueeze(0))
    interpolated = interpolated.view(-1, *inputs.shape[1:])  # Flatten batch
    interpolated.requires_grad_(True)
    
    # Forward pass through model
    outputs = model(interpolated)
    target_outputs = outputs[:, target_class_idx]
    
    # Compute gradients
    gradients = torch.autograd.grad(
        outputs=target_outputs.sum(),
        inputs=interpolated,
        create_graph=False,
        retain_graph=False
    )[0]
    
    # Reshape and average over integration steps
    gradients = gradients.view(steps, *inputs.shape)
    avg_gradients = gradients.mean(dim=0)
    
    # Compute integrated gradients
    integrated_grads = (inputs - baseline) * avg_gradients
    
    return integrated_grads
    </div>
    
    <p><strong>Implementation in ReporterAgent:</strong></p>
    <ul style="margin-left: 1.5rem; line-height: 1.6;">
        <li>‚Ä¢ Visualizes attention maps in bidding documents</li>
        <li>‚Ä¢ Generates relevance heatmaps for unstructured text</li>
        <li>‚Ä¢ Enables complete traceability of conclusions</li>
        <li>‚Ä¢ Identifies suspicious patterns in natural language</li>
    </ul>
</div>

<!-- Unified Framework -->
<h4 style="color: var(--text-primary); margin: 2rem 0 1rem 0;">üéØ Unified Explainability Framework</h4>

<div style="background: var(--bg-secondary); border: 2px solid var(--border); border-radius: 1rem; padding: 2rem;">
    <p style="margin-bottom: 1.5rem;"><strong>Formal Pipeline:</strong> E: X √ó F √ó C ‚Üí XÃÉ</p>
    
    <div class="axiom-list">
        <p><strong>Explanation Quality Metrics:</strong></p>
        <div style="margin: 1rem 0; line-height: 1.8;">
            <p><strong>1. Fidelity:</strong> œÅ(E, f) = ùîº<sub>x</sub>[‚Äñf(x) - fÃÉ<sub>E</sub>(x)‚Äñ¬≤]</p>
            <p><strong>2. Stability:</strong> œÉ(E) = ùîº[‚ÄñE(x + Œµ) - E(x)‚Äñ / ‚ÄñŒµ‚Äñ]</p>
            <p><strong>3. Complexity:</strong> Œ∫(E) = |{features: |œÜ·µ¢| > œÑ}|</p>
            <p><strong>4. Causal Coherence:</strong> Œ≥(E) = P(E satisfies causal constraints)</p>
        </div>
    </div>
    
    <div style="display: grid; gap: 1.5rem; margin-top: 2rem;">
        <div style="background: linear-gradient(135deg, #f0f9ff, #e0f2fe); border: 1px solid #0284c7; border-radius: 0.5rem; padding: 1.5rem;">
            <h5 style="color: #0c4a6e; margin-bottom: 0.5rem;">Level 1: Global Explanation</h5>
            <p><strong>Permutation Importance:</strong> I(f<sub>j</sub>) = ùîº[L(Y, f(X))] - ùîº[L(Y, f(X<sup>œÄ<sub>j</sub></sup>))]</p>
            <p><strong>Partial Dependence:</strong> fÃÑ<sub>S</sub>(x<sub>S</sub>) = ùîº<sub>X<sub>C</sub></sub>[f(x<sub>S</sub>, X<sub>C</sub>)]</p>
        </div>
        
        <div style="background: linear-gradient(135deg, #f0fdf4, #dcfce7); border: 1px solid #16a34a; border-radius: 0.5rem; padding: 1.5rem;">
            <h5 style="color: #14532d; margin-bottom: 0.5rem;">Level 2: Local Explanation</h5>
            <p>SHAP values with complete axiomatic guarantees</p>
            <p>Optimal counterfactuals via convex programming</p>
            <p>Anchors with guarantees: P(f(x) = f(a(x))) ‚â• 1-Œ¥</p>
        </div>
        
        <div style="background: linear-gradient(135deg, #fef3c7, #fde68a); border: 1px solid #f59e0b; border-radius: 0.5rem; padding: 1.5rem;">
            <h5 style="color: #78350f; margin-bottom: 0.5rem;">Level 3: Semantic Explanation</h5>
            <p><strong>Template-based:</strong> g: XÃÉ ‚Üí L (natural language)</p>
            <p><strong>Constraints:</strong> coherence(g(xÃÉ)) ‚àß correctness(g(xÃÉ)) ‚àß completeness(g(xÃÉ))</p>
        </div>
    </div>
</div>

<!-- Performance Guarantees -->
<div style="background: linear-gradient(135deg, #fef2f2, #fee2e2); border: 2px solid #ef4444; border-radius: 1rem; padding: 2rem; margin: 2rem 0;">
    <h4 style="color: #991b1b; margin-bottom: 1.5rem;">Theoretical Guarantees and Performance Bounds</h4>
    <div class="axiom-list">
        <p><strong>Theorem (Sample Complexity for LIME Explanations):</strong></p>
        <p>To obtain Œµ-accurate explanations with probability 1-Œ¥, we need:</p>
        <div style="text-align: center; margin: 1.5rem 0; font-family: 'Times New Roman', serif;">
            n ‚â• (2/Œµ¬≤) √ó [VC(H) √ó log(2/Œµ) + log(2/Œ¥)]
        </div>
        <p>where VC(H) is the Vapnik-Chervonenkis dimension of the hypothesis space.</p>
        
        <p style="margin-top: 1.5rem;"><strong>Approximation Bound (Fidelity):</strong></p>
        <div style="margin: 1rem 0;">
            ‚Äñf(x) - g(x)‚Äñ<sub>œÄ</sub> ‚â§ Œµ with probability ‚â• 1 - Œ¥
        </div>
        <p>if |D| ‚â• O((d/Œµ¬≤) √ó log(d/Œ¥)), where d is local dimensionality.</p>
        
        <p style="margin-top: 1.5rem;"><strong>Guaranteed Time Complexity:</strong></p>
        <ul style="margin-left: 1.5rem; line-height: 1.6;">
            <li>‚Ä¢ <strong>SHAP:</strong> O(2·µà) exact, O(M) approximate with M samples</li>
            <li>‚Ä¢ <strong>LIME:</strong> O(Md + d¬≥) for M samples, d features</li>
            <li>‚Ä¢ <strong>Integrated Gradients:</strong> O(kd) for k integration steps</li>
        </ul>
    </div>
</div>